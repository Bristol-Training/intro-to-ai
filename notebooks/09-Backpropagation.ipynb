{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# What is backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Now that we've explored how adjusting weights and biases affects the loss function, it's time to delve into how neural networks systematically learn these optimal parameters. This is where the **backpropagation** algorithm comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Backpropagation, short for \"backward propagation of errors,\" is a fundamental algorithm used to train neural networks. It efficiently computes the gradient of the loss function with respect to each parameter (weights and biases) in the network. These gradients indicate the direction and rate at which each parameter should be adjusted to minimize the loss.\n",
    "\n",
    "This process is repeated iteratively, allowing the network to learn the optimal parameters that minimize the loss function.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The steps of the backpropagation algorithm are as follows:\n",
    "\n",
    "1. Forward pass: Input data is passed through the network to compute the predictions. The loss function is used to measure how far the predictions are from the true outputs.\n",
    "\n",
    "2. Compute gradients (Backward pass): Starting from the output layer, we compute the gradient of the loss with respect to the output. We then recursively compute gradients for each previous layer (i.e., how the loss changes with respect to the weights and biases in each layer).\n",
    "\n",
    "3. Update parameters: Once we have the gradients, we update the weights and biases using gradient descent:\n",
    "\n",
    "$w \\leftarrow w - l \\frac{\\partial f}{\\partial w}, \\quad b \\leftarrow b - l \\frac{\\partial f}{\\partial b}$\n",
    "\n",
    "where $l$ is the learning rate and $f$ is the loss function.\n",
    "\n",
    "Larger learning rates mean you might converge faster to a minimum but you have a chance of jumping *over* a minimum. Smaller learning rates mean you are less likely to overshoot but might converge more slowly/get stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Other optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In practise, modern optimisers can perform better than standard gradient descent. The general principles are the same, with some clever maths under the hood. Some of these methods include \"Stochastic Gradient Descent\" (SGD) and \"Momentum Gradient Descent\" (MGD) more info on these can be found here: \n",
    "- [Standard GD](https://www.youtube.com/watch?v=JdeemZDr-hU&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=5)\n",
    "- [SGD](https://www.youtube.com/watch?v=VbYTp0CIJkY&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=5)\n",
    "- [MGD](https://www.youtube.com/watch?v=qfb2ezDWGIU&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "For a fun visualise of a neural network training (and the associated decision boundary warping around points), you can have a play with the [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.61021&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
