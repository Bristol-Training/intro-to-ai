{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "\n",
    "This section builds a data pipeline which includes data loading, preprocessing, and batching with DataLoader. We will use the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Load and explore the Iris dataset\n",
    "\n",
    "The [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) is a classic dataset in machine learning practice containing measurements of sepals and petals from three species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# extract features and target classes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "# Print the first few rows of the dataset to check its structure\n",
    "print(iris_df.head())\n",
    "\n",
    "# print to check the overall structure of our dataset\n",
    "# and also to find how many classes we have\n",
    "print(f\"Dataset dimensions: {X.shape}\")\n",
    "print(f\"Target classes: {target_names}\")\n",
    "print(f\"Feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We have known that we have 150 samples and 4 features in our dataset, now let us visualize the relationships between these features using a pair plot.\n",
    "Additionally, we can also check the correlation matrix of the features to see how strongly the features are correlated with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot to visualize relationships between features\n",
    "sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"], palette=\"Set2\")\n",
    "plt.suptitle('Pair Plot of Iris Dataset', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# check the correlation matrix of the features\n",
    "corr_matrix = iris_df[feature_names].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Iris Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 2: Split data into training and testing sets\n",
    "\n",
    "We now divide our data into training and testing datasets in 80:20 ratio. This means, we will be using 80% of our data for training and 20% for evaluating the model's performance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets with a seed for reproducibility\n",
    "# X_train here contains training set for feature data\n",
    "# y_train here contains target labels for training set, or what we want to predict, or the ground truth\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: Standarise or scale the feature data\n",
    "\n",
    "Networks generally work better when the numbers are all the same order of magnitude. We want the network to learn how numbers vary, not their relative sizes to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the feature data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# learn the parameter from training data and fit a transformer to it\n",
    "# fit() - computes mean and std deviation to scale\n",
    "# transform() - used to scale using mean and std deviation calculated using fit()\n",
    "# fit_transform() - combination of both fit() and transform()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# no fit() as we want to avoid data leakage\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now let us convert feature matrices to FloatTensor (tensor type for numerical data) and LongTensor (tensor type for \"long\" which is just a type of integer labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 4: Create tensor dataset and [data loader](https://www.eletreby.me/blog/getting-started-with-pytorch-dataset-and-dataloader) for batch training\n",
    "\n",
    "Whilst it is possible to use plain tensors for your training set, it can be advantageous to make use of PyTorch's existing mechanisms for loading in data. This is particuarly relevant when we want to handle large amounts of data in effecient ways with multiple GPUs (e.g. using some kind of server or high performance computer). Below initialise a `TensorDataset` class for defining and accessing our data in an efficient way (we'll talk about what a class is in the next section). The `DataLoader` class wraps the `Dataset` class and handles batching, shuffling, and utilise Python's multiprocessing to speed up data retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels into a single dataset\n",
    "batch_size = 30\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print batch information\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Finally, our dataset is ready for model definition, training, and evaluation.\n",
    "\n",
    "Next section will explain the model that we will utilise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
