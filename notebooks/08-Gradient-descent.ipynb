{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Gradient descent: How do neural networks learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We have seen how you can prepare some data with features ``X`` and labels ``y``. Then we can define the architecture/structure of a multi-layer perceptron (MLP) and randomly initialise a collection of weights ``w`` and biases ``b`` for the different neurons and layers of our MLP. We can then feed our training data through our MLP to get predictions of our class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "$X_{i} \\to MLP(w, b) \\to Prediction_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We have also seen that we can measure our mean squared error by adding up the squared difference between each prediction and the corresponding observation (i.e. the correct answer) and then dividing by the number of predictions. This value will be ``0`` if our predictions are all correct and will get larger as a larger proportion of our guesses are wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "$Predictions, Observations \\to Error$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Tuning our weights and biases\n",
    "\n",
    "We saw that randomly initialising the weights and biases was unlikely to lead to good predictions which means our error will start off quite high. We want to tune our weights and biases in a way that reduces that error.\n",
    "\n",
    "The error is a function of our predictions and our observations. Our predictions are a function of our weights and biases. These statements together imply that our error is implicitly a function of our weights and biases. In other words:\n",
    "\n",
    "*Changing your weights and biases in your neural network will change your error.*\n",
    "\n",
    "For fixed data and network structure, we can try to understand how we can change our weights and biases to reduce our error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Gradient Descent in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearFunc(x, weight, bias):\n",
    "    \"\"\"\n",
    "    Linear function: y = weight * x + bias\n",
    "    \"\"\"\n",
    "    return weight * x + bias\n",
    "\n",
    "import math\n",
    "# Generate some sample data\n",
    "x = [i for i in range(10)]\n",
    "y = [math.sin(i) for i in x]\n",
    "\n",
    "def MSE(x, y, weight, bias):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error (MSE) for a linear model.\n",
    "    \"\"\"\n",
    "    return sum((y[i] - LinearFunc(x[i], weight, bias)) ** 2 for i in range(len(y))) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Below you can pick different weights and biases for your linear model and see how they affect the MSE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weight = 1\n",
    "bias = 0\n",
    "Linear = [LinearFunc(i, weight, bias) for i in x]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, marker='o', label='Sine Function', color='blue')\n",
    "plt.plot(x, Linear, label=f'y = {weight}x+{bias}', color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title(f'Sine Function vs. Linear Model; y = {weight}x + {bias}; MSE = {MSE(x, y, weight, bias):.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now if we fix the bias but vary the weight we can see how that affects the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the bias but vary the weight\n",
    "bias = 2\n",
    "\n",
    "ErrorDict = {}\n",
    "for weight in range(-20, 20, 2):\n",
    "    mse = MSE(x, y, weight, bias)\n",
    "    ErrorDict[weight] = mse\n",
    "\n",
    "# Plot the error curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ErrorDict.keys(), ErrorDict.values(), marker='o')\n",
    "plt.title('Mean Squared Error (MSE) vs. Weight (w1)')\n",
    "plt.xlabel('Weight (w1)')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We can see for different weight values, there is a different error value. Accurate prediction is as simple as minimising the error with respect to the weights and biases.\n",
    "\n",
    "In a calculus class you might have taken the derivative of a function $\\frac{dError}{dWeight}$, which represents the slope/gradient of the error function with respect to the weight at a given position. If you're unfamiliar or rusty with calculus, you might want a refresher with something like YouTuber [3Blue1Brown's Essence of Calculus Course](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).\n",
    "\n",
    "Whilst you could in theory solve for when the derivative/slope is zero, to find an explicit answer in situations where there are a large number of weights and biases is often intractably difficult.\n",
    "\n",
    "Instead, we understand that if the derivative is positive when the slope is upwards and negative when the slope is downwards, we can just head in the opposite direction of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DerivativeMSE(x, y, weight, bias):\n",
    "    \"\"\"\n",
    "    Calculate the derivative of MSE with respect to weight.\n",
    "    \"\"\"\n",
    "    return -2 * sum((y[i] - LinearFunc(x[i], weight, bias)) * x[i] for i in range(len(y))) / len(y)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "# Plot MSE\n",
    "axes[0].plot(ErrorDict.keys(), ErrorDict.values(), marker='o')\n",
    "axes[0].set_title('MSE vs. Weight (w1)')\n",
    "axes[0].set_xlabel('Weight (w1)')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Derivative of MSE\n",
    "axes[1].plot(ErrorDict.keys(), [DerivativeMSE(x, y, weight, bias) for weight in ErrorDict.keys()], marker='o')\n",
    "axes[1].set_title('Derivative of MSE vs. Weight (w1)')\n",
    "axes[1].set_xlabel('Weight (w1)')\n",
    "axes[1].set_ylabel('d(MSE)/dw1')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Consider the above left plot and choose a position on the x-axis and look at its corresponding MSE value. You can intuitively see which way is \"downhill\". \n",
    "\n",
    "On the above right plot you should be able to read off the derivative at the same x-axis value. If the left plot shows you're facing uphill, the derivative should be positive on the right plot. If you're facing downhill the derivative should be negative. The larger the value of the derivative, the steeper the slope.\n",
    "\n",
    "Imagine you're hiking in a misty mountain range and trying to get back to base camp at the bottom of the range, but you can't see in front of you. All you can feel is the slope underneath your feet. What you might want to do is to feel the slope underneath you (i.e. the gradient of the ground) and take a step in the direction of that slope. Once you take a step, you stop, feel the ground underneath you again and decide on a new direction and step size.\n",
    "\n",
    "If the slope is very big, you might take a larger step, assuming it will get you downhill faster. If the slope is very small you might take a smaller step so you don't overshoot and start going uphill again. This is how gradient descent works!\n",
    "\n",
    "In practise we take a step size proportional to the derivative/slope. We call the constant of proportionality the *learning rate*. If the learning rate is too large we might step over a minimum. If the rate is too small then we might take too long to find the minimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Gradient Descent in Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "In practise, we want to update all the weights and biases of our neural network at once. This means we want to use a generalisation of the derivative called the gradient function. Given a function $f$ of $n$ variables: $w_{1}, w_{2}, \\ldots, w_{n}$ we can define the gradient function of $f$ as:\n",
    "\n",
    "$\\nabla f (w_{1}, \\ldots, w_{n}) = \\begin{bmatrix}\n",
    "           \\frac{\\partial f}{\\partial w_{1}} \\\\\n",
    "           \\frac{\\partial f}{\\partial w_{2}} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{\\partial f}{\\partial w_{n}}\n",
    "         \\end{bmatrix}$\n",
    "\n",
    "where $\\frac{\\partial f}{\\partial w_{1}}$ is the *partial derivative* of the function $f$ with respect to $w_{1}$. What this means is that each component of this vector tells you how steep the function $f$ is with respect to each of its variables.\n",
    "\n",
    "This just now means, if we think of our misty mountain range analogy from earlier, instead of only having to figure out if we need to walk north/south and east/west, we have a lot more directions to consider in gradient descent! The principle remains the same, even if visualising the geometry gets a little hazy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "You can play around with this web app to see how the loss surface \"looks\" with respect to a function and see how gradient descent moves you downhill towards minimal loss \n",
    "\n",
    "https://neuralpatterns.io/hill_climber.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Other optimisers\n",
    "\n",
    "In practise, modern optimisers can perform better than standard gradient descent which often has problems getting \"stuck\" in areas that are minima but not the global minimum (imagine walking down hill until you get into a valley, or onto a little plateau which is not the bottom of the misty mountain range).\n",
    "\n",
    "The general principles of these optimisers rely on gradient descent as a base line, but they are modified somewhat to get around this problem of getting stuck in local minima and to reduce the computational load of calculating gradients. You could think of the difference between standard gradient descent and stochastic gradient descent as that between a scout guide carefully working out the precise way to step every single time versus a bumbling hiker who is hastily making decisions about which way to walk every step. Their decisions are quicker to make and less effected by small irregularities of the mountain (and in practise, at least for neural networks, tend to perform better!)\n",
    "\n",
    "Some of these methods include \"Stochastic Gradient Descent\" (SGD) and \"Momentum Gradient Descent\" (MGD) more info on these can be found here: \n",
    "\n",
    "- [Standard GD](https://www.youtube.com/watch?v=JdeemZDr-hU&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=5)\n",
    "- [SGD](https://www.youtube.com/watch?v=VbYTp0CIJkY&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=5)\n",
    "- [MGD](https://www.youtube.com/watch?v=qfb2ezDWGIU&list=PLqwozWPBo-FtJ1wfHq47F__ReKfmGLUZP&index=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
