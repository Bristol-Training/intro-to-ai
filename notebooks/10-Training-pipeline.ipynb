{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron Implementation\n",
    "---------------------------------------\n",
    "A [multi-layer perceptron](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning) is a type of [feedforward neural network (FNN)](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network). It is composed of fully connected layers and non-linear activation functions, and is commonly used for classifying data that cannot be separated by a straight line.\n",
    "\n",
    "![MLP](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "### Input layer:\n",
    "Each neuron represents a feature (e.g. petal length). This layer passes data forward without performing computation.\n",
    "\n",
    "### Hidden layer:\n",
    "Hidden layers form the core of the neural network. Neurons:\n",
    "\n",
    "- Receive inputs from all previous-layer neurons (fully connected)\n",
    "- Apply a weighted sum and a non-linear activation function (like ReLU)\n",
    "- Learn by updating weights during training\n",
    "\n",
    "### Output layer:\n",
    "Generates the prediction. For classification tasks like the Iris dataset, each neuron can represent a class (e.g. setosa, versicolor, virginica).\n",
    "\n",
    "\n",
    "Check out [Neural Network Playground](https://playground.tensorflow.org/) to experiment with layers, activations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Define the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initialise a simple feedforward MLP architecture.\n",
    "        \n",
    "        Parameters:\n",
    "         input_size: Number of input features (e.g., 4 for Iris dataset)\n",
    "         hidden_size: Number of neurons in the hidden layer\n",
    "         num_classes: Number of output classes (e.g., 3 for Iris species)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # First layer (input to hidden)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second layer (hidden to hidden)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer (hidden to output)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass through the network for a single input.\n",
    "        \n",
    "        Parameter:\n",
    "         x: Input tensor of shape [input_size] representing a single sample\n",
    "        \n",
    "        Returns:\n",
    "         Output tensor of shape [num_classes] for a single prediction\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass through the network\n",
    "        # Each step applies a linear transformation followed by a non-linear activation\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)  # Apply softmax to get probabilities\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Set model parameters and initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4    # Assuming 4 features (like Iris dataset)\n",
    "hidden_size = 16  # Neurons in hidden layer\n",
    "num_classes = 3   # Output classes \n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, X, y_true):\n",
    "    \"\"\"\n",
    "    Calculate loss for the model without training.\n",
    "    \n",
    "    Parameters:\n",
    "     model: The MLP model\n",
    "     X: Input features (torch tensor) - single sample, not batched\n",
    "     y_true: True label (torch tensor) - single label, not batched\n",
    "    \n",
    "    Returns:\n",
    "     loss: Sum of squared errors loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add batch dimension of 1 for model compatibility\n",
    "    X_input = X.unsqueeze(0)  # Adds batch dimension [1, input_features]\n",
    "    y_pred = model(X_input)   # y_pred shape: [1, num_classes]\n",
    "    \n",
    "    # Get number of classes from model's output layer\n",
    "    num_classes = model.output.out_features\n",
    "    \n",
    "    # Create one-hot encoded label for a single sample\n",
    "    \"\"\"\n",
    "    as we are working with a classification problem that has 3 possible classes (0, 1, and 2). \n",
    "    Using one-hot encoding gives us:\n",
    "\n",
    "    Class 0 becomes: [1, 0, 0]\n",
    "    Class 1 becomes: [0, 1, 0]\n",
    "    Class 2 becomes: [0, 0, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    y_true_one_hot = torch.zeros(1, num_classes)\n",
    "    \n",
    "    # Convert label tensor to integer and set the appropriate position to 1\n",
    "    label_idx = y_true.item()\n",
    "    y_true_one_hot[0, label_idx] = 1\n",
    "    \n",
    "    # Calculate sum of squared errors between prediction and one-hot label\n",
    "    squared_errors = (y_pred - y_true_one_hot) ** 2\n",
    "    loss = torch.sum(squared_errors)\n",
    "    \n",
    "    # Calculate and print the loss value\n",
    "    loss_value = loss.item()\n",
    "    \n",
    "    # Return the loss as a Python float\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Iris Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, the process is identical to the data handling steps in \"DataPipeline\" notebook.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "# extract features and target classes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# standardise the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Test Loss and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialise a list to store the individual losses\n",
    "losses = []\n",
    "# Evaluate the average SSE loss of the model on the test_dataset\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "num_test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        for i in range(features.size(0)):\n",
    "            # Extract individual feature and label\n",
    "            single_feature = features[i]\n",
    "            single_label = labels[i]\n",
    "\n",
    "            # Calculate loss for individual sample\n",
    "            loss = calculate_loss(model, single_feature, single_label)\n",
    "            losses.append(loss)\n",
    "            total_test_loss += loss\n",
    "            num_test_samples += 1\n",
    "\n",
    "# Calculate average loss across all processed samples\n",
    "if num_test_samples > 0:\n",
    "    avg_test_loss = total_test_loss / num_test_samples\n",
    "    print(f\"\\nAverage SSE loss on test set ({num_test_samples} samples): {avg_test_loss:.4f}\")\n",
    "    \n",
    "    # Plot the losses as a line chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(num_test_samples), losses, label='Sample-wise SSE Loss', color='tab:blue', linewidth=2)\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"SSE Loss\")\n",
    "    plt.title(\"SSE Loss per Sample on Test Set\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "# Step 6: Implement Training Pipeline\n",
    "\n",
    "### Setting the learning Rate\n",
    "\n",
    "Ineuralep network trainingsettingng the learning rate over time is beneficial. With high learning rates, parameter vectors bounce chaotically, struggling to settle into optimal regions of the loss function. The timing of decay is crucial; too slow wastes computation, too fast prevents finding the best solution. The code below `optim.SGD(model.parameters(), lr=0.01)` creates a [Stochastic Gradient Descent optimiser](https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent) that will adjust the parameters of our model during training. It configures the optimiser to work with all the parameters in our model and sets the initial learning rate to `0.01`. This optimiser SGD is responsible for updating our model's weights based on the calculated gradients, helping the model to improve over time during the training process.er training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set up optimiser which in our case is Stochastic Gradient Descent\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# nn.Module has a built in method called .parameters() which returns the model's parameters\n",
    "# Related batches to learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Choosing number of epochs\n",
    "Determining the optimal [number of epochs](https://deepai.org/machine-learning-glossary-and-terms/epoch) is crucial for neural network design. Too few, and our model will not learn properly; too many, and it may overfit, learning data noise rather than useful patterns. The right number is typically found through experimentation and monitoring performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# What is an epoch? One run of all the batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "As an exercise, you can try increasing or decreasing the number of epochs to analyse how it affects our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []  \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ### TRAINING PHASE\n",
    "    model.train()  # Set to training mode\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(features) \n",
    "\n",
    "        # create one-hot encoded labels into a format that allows us to calculate the loss\n",
    "        one_hot = torch.zeros_like(outputs).scatter_(1, labels.unsqueeze(1), 1)\n",
    "        loss = torch.sum((outputs - one_hot) ** 2)  # SSE loss\n",
    "        \n",
    "        loss.backward() # computes gradients of the loss with respect to model parameters using backpropagation\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    ### TESTING PHASE - Evaluate after each epoch\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    test_total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for testing\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)\n",
    "            one_hot = torch.zeros_like(outputs).scatter_(1, labels.unsqueeze(1), 1)\n",
    "            test_loss = torch.sum((outputs - one_hot) ** 2) # SSE loss\n",
    "            test_total_loss += test_loss.item()\n",
    "    \n",
    "    avg_test_loss = test_total_loss / len(test_loader.dataset)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Every 10 epochs, prints a progress update showing the current epoch number and the average training and test losses\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plot training and testing loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average SSE Loss\")\n",
    "plt.title(\"Training and Testing Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "And for classification problems it's good practise to visualise a confusion matrix to see how well our classifier predicts each class. We are using a very small, clean dataset here so it's not surprising that our MLP performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for features, _ in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        y_pred.extend(predicted.numpy())\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
