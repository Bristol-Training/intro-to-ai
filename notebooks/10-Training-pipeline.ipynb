{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Training pipeline: Multi-layer perceptron implementation\n",
    "\n",
    "A [multi-layer perceptron](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning) is a type of [feedforward neural network (FNN)](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network). It is composed of fully connected layers and non-linear activation functions, and is commonly used for classifying data that cannot be separated by a straight line.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "### Input layer\n",
    "Each neuron represents a feature (e.g. petal length). This layer passes data forward without performing computation.\n",
    "\n",
    "### Hidden layer\n",
    "Hidden layers form the core of the neural network. Neurons:\n",
    "\n",
    "- Receive inputs from all previous-layer neurons (fully connected)\n",
    "- Apply a weighted sum and a non-linear activation function (like ReLU)\n",
    "- Learn by updating weights during training\n",
    "\n",
    "### Output layer\n",
    "Generates the prediction. For classification tasks like the Iris dataset, each neuron can represent a class (e.g. setosa, versicolor, virginica).\n",
    "\n",
    "\n",
    "Check out [Neural Network Playground](https://playground.tensorflow.org/) to experiment with layers, activations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Define the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initialise a simple feedforward MLP architecture.\n",
    "        \n",
    "        Parameters:\n",
    "         input_size: Number of input features (e.g., 4 for Iris dataset)\n",
    "         hidden_size: Number of neurons in the hidden layer\n",
    "         num_classes: Number of output classes (e.g., 3 for Iris species)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # First layer (input to hidden)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second layer (hidden to hidden)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer (hidden to output)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass through the network for a single input.\n",
    "        \n",
    "        Parameter:\n",
    "         x: Input tensor of shape [input_size] representing a single sample\n",
    "        \n",
    "        Returns:\n",
    "         Output tensor of shape [num_classes] for a single prediction\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass through the network\n",
    "        # Each step applies a linear transformation followed by a non-linear activation\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)  # Apply softmax to get probabilities\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Set model parameters and initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4    # Assuming 4 features (like Iris dataset)\n",
    "hidden_size = 16  # Neurons in hidden layer\n",
    "num_classes = 3   # Output classes \n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Iris Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, the process is identical to the data handling steps in \"DataPipeline\" notebook.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "# extract features and target classes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# standardise the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Test Loss and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize a list to store the individual losses\n",
    "losses = []\n",
    "\n",
    "# Evaluate the average MSE loss of the model on the test_dataset\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "num_test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        for i in range(features.size(0)):\n",
    "            # Extract individual feature and label\n",
    "            single_feature = features[i].unsqueeze(0)  # Add batch dimension\n",
    "            single_label = labels[i]\n",
    "\n",
    "            # Forward pass\n",
    "            prediction = model(single_feature)\n",
    "\n",
    "            # One-hot encode label and convert to float\n",
    "            one_hot_label = F.one_hot(single_label, num_classes=3).float().unsqueeze(0)\n",
    "\n",
    "            # Calculate MSE loss\n",
    "            loss = F.mse_loss(prediction, one_hot_label)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            total_test_loss += loss.item()\n",
    "            num_test_samples += 1\n",
    "\n",
    "# Calculate average loss across all processed samples\n",
    "if num_test_samples > 0:\n",
    "    avg_test_loss = total_test_loss / num_test_samples\n",
    "    print(f\"\\nAverage MSE loss on test set ({num_test_samples} samples): {avg_test_loss:.4f}\")\n",
    "\n",
    "    # Plot the losses as a line chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(num_test_samples), losses, label='Sample-wise MSE Loss', color='tab:blue', linewidth=2)\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"MSE Loss per Sample on Test Set\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Step 5: Implement Training Pipeline\n",
    "\n",
    "### Setting the learning Rate\n",
    "\n",
    "Ineuralep network trainingsettingng the learning rate over time is beneficial. With high learning rates, parameter vectors bounce chaotically, struggling to settle into optimal regions of the loss function. The timing of decay is crucial; too slow wastes computation, too fast prevents finding the best solution. The code below `optim.SGD(model.parameters(), lr=0.01)` creates a [Stochastic Gradient Descent optimiser](https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent) that will adjust the parameters of our model during training. It configures the optimiser to work with all the parameters in our model and sets the initial learning rate to `0.01`. This optimiser SGD is responsible for updating our model's weights based on the calculated gradients, helping the model to improve over time during the training process.er training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set up optimiser which in our case is Stochastic Gradient Descent\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# nn.Module has a built in method called .parameters() which returns the model's parameters\n",
    "# Related batches to learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Choosing number of epochs\n",
    "Determining the optimal [number of epochs](https://deepai.org/machine-learning-glossary-and-terms/epoch) is crucial for neural network design. Too few, and our model will not learn properly; too many, and it may overfit, learning data noise rather than useful patterns. The right number is typically found through experimentation and monitoring performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# What is an epoch? One run of all the batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "As an exercise, you can try increasing or decreasing the number of epochs to analyse how it affects our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ### TRAINING PHASE\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        # One-hot encode labels and convert to float\n",
    "        one_hot = torch.zeros_like(outputs).scatter_(1, labels.unsqueeze(1), 1).float()\n",
    "        loss = F.mse_loss(outputs, one_hot)  # MSE loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item() * features.size(0)  # Multiply by batch size to undo mean\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    ### TESTING PHASE\n",
    "    model.eval()\n",
    "    test_total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)\n",
    "            one_hot = torch.zeros_like(outputs).scatter_(1, labels.unsqueeze(1), 1).float()\n",
    "            test_loss = F.mse_loss(outputs, one_hot)\n",
    "            test_total_loss += test_loss.item() * features.size(0)  # Accumulate total\n",
    "\n",
    "    avg_test_loss = test_total_loss / len(test_loader.dataset)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plot training and testing loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average MSE Loss\")\n",
    "plt.title(\"Training and Testing Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "And for classification problems it's good practise to visualise a confusion matrix to see how well our classifier predicts each class. We are using a very small, clean dataset here so it's not surprising that our MLP performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for features, _ in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        y_pred.extend(predicted.numpy())\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
