{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c4a454-d0f4-4d66-af7b-f59703d7f53a",
   "metadata": {},
   "source": [
    "# Basics of installing PyTorch (CUDA) in Anaconda\n",
    "- Open Anaconda Powershell Prompt\n",
    "- Create new virtual environment: conda create -n py312 python=3.12\n",
    "- Activate it: conda activate py312\n",
    "- Install PyTorch using CONDA: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "- Verify PyTorch installation: python -c \"import torch; print(torch.__ version__)\"\n",
    "- Verify CUDA availability: python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "### Alternative PyTorch\n",
    "- Install PyTorch for CPU: conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4fdeb-283d-45ab-a3c1-5fc898500cec",
   "metadata": {},
   "source": [
    "## Tips to redirect your Jupyter Notebook kernel to the new environment\n",
    "Activate your virtual environment first on Anaconda Powershell Prompt\n",
    "- #### Install ipykernel\n",
    "conda install ipykernel\n",
    "\n",
    "- #### Add the environment to Jupyter (e.g. if your virtual environment name is py312)\n",
    "python -m ipykernel install --user --name=py312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2df348-a993-4323-89dd-40510fafa5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if PyTorch exists otherwise follow the above steps to install PyTorch\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e82fc-949c-403e-b4e2-42de07a5502c",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "-------------------------------------------\n",
    "A tensor can be viewed as a multi-dimensional array. Similar to how an n-dimensional vector is shown as a one-dimensional array with _n_ elements relative to a specific basis, any tensor can be expressed as a multi-dimensional array when referenced to a basis. The individual values within this multi-dimensional structure are referred to as the tensor's components.\n",
    "\n",
    "[PyTorch](https://pytorch.org/foundation) is an open-source machine learning library developed by Facebook's AI Research lab. It's known for its flexibility, intuitive design, and dynamic computational graph which makes debugging easier.\r\n",
    "his  library offers multi-dimensional tensor data structures and implements various mathematical functions to manipulate these tensors. It also includes numerous tools for effective tensor serialisation, handling arbitrary data types, and provides several other practical utilities.\n",
    "\n",
    "PyTorch shares significant similarities with NumPy, though it uses the term ''tensor'' instead of ''N-dimensional array''. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5e8928-5f2d-42a3-be42-5717dd3f64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "array_np = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "array_pytorch = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6]])\n",
    "print(array_np)\n",
    "print(array_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fab10-39c0-400b-816d-78b04f245f70",
   "metadata": {},
   "source": [
    "Now let us create tensors in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4953c722-d171-43e1-9aa7-e5b3a7483e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4]) # This creates a 1-dimensional tensor (vector) with 4 elements\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6884768c-9088-4bdc-826f-d24b6bb86e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8318, 0.4992],\n",
       "        [0.3006, 0.1118]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create specific tensors\n",
    "zeros = torch.zeros(3, 4)  # 3x4 tensor of zeros\n",
    "ones = torch.ones(2, 3)    # 2x3 tensor of ones\n",
    "rand = torch.rand(2, 2)    # 2x2 tensor of random numbers (0-1)\n",
    "\n",
    "rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28249cf4-3a49-45b7-a4fb-ccfa2345a18a",
   "metadata": {},
   "source": [
    "Let us now explore some common [tensor operations](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cdd9025-85ab-4d32-85e7-a03626217eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tensors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7b7f5b0-99a9-42ac-a5a2-5d9815d41f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape # dimension of the tensor a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19e4049d-86df-4275-aa98-71ee3b26ca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8371,  0.0040, -0.1456, -0.6450,  1.7360,  1.6073,  0.8404,  1.4880],\n",
       "        [-1.3423,  0.0705,  0.8875,  0.2284, -0.2351, -0.8133, -0.3248,  1.1377]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn(4, 4)  # creates a 4x4 tensor with random values\n",
    "c.view(2, 8)  # reshape to 2x8 tensor / view() reshapes a tensor without changing its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfd4a3bd-d5f5-4aed-9a61-2020894004be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn((1, 2, 3, 4, 5))\n",
    "c.squeeze().shape  # squeeze remove dimensions of size one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "522234ce-9dc4-4e15-8428-c86a442968cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4, 5, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(dim=5).shape  # unsqueeze adds a new dimension one at dimensional index `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbd65a96-4f53-4039-895e-6631f48ceb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b # element-wise addition of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84495d09-0ff4-470e-a692-ed5fff68b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b # element-wise multiplication of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d76ed65-eb56-4b9d-a900-1bf7f8163476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b  # matrix multiplication (note: we have 1-dimensional matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94c03cb3-e7f8-445c-a902-72c3cd55969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / b  # element-wise division of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cb2d6da-4d2e-42c9-a0ca-69a99e2b7e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(a, b)  # scalar product of two vectors / element-wise multiplication followed by addition of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9ca5f-2dc8-4a6c-b711-318db5c592b6",
   "metadata": {},
   "source": [
    "**Exercise: can you guess why scalar product and matrix multiplication are producing same answer ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3a656-4d05-48c0-b7fe-bcbcee088e81",
   "metadata": {},
   "source": [
    "A parallel [PyTorch CUDA](https://pytorch.org/docs/2.5/cuda.html) version is also available, allowing you to execute tensor calculations on NVIDIA GPUs that have a compute capability of 3.0 or higher. But in this course, as time may not permit, we will be restricted to pre-definied dataset. In future, your project might need CUDA acceleration for which please install the CUDA version of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0520-6a94-4be8-ba6d-418625907e48",
   "metadata": {},
   "source": [
    "# Classes\n",
    "----------------------------------------------\n",
    "Classifications (classes) usually refers to catergories or labels our neural network is supposed to predict.\n",
    "\n",
    "It can be of two types: \n",
    "- binary classification (yes or no/malignant or benign/dog or cat etc.) or\n",
    "- multi-class classification (cat or dog or capibara/digit recognition/species of flowers etc.) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284578d5-33a1-4f05-80cd-76a7888ccce3",
   "metadata": {},
   "source": [
    "### Example 1: Binary classification\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b1acac5-7bcb-46ef-9477-8a5db803b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails and their classifications:\n",
      "Email 1: Promotional words: True, Suspicious links: True, Known sender: False → Classification: Spam\n",
      "Email 2: Promotional words: False, Suspicious links: False, Known sender: True → Classification: Not Spam\n",
      "Email 3: Promotional words: True, Suspicious links: False, Known sender: True → Classification: Not Spam\n",
      "Email 4: Promotional words: True, Suspicious links: True, Known sender: False → Classification: Spam\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification - Email Spam Detection\n",
    "\n",
    "# Simplified representation of email features\n",
    "emails = [\n",
    "    {\"id\": 1, \"contains_promotional_words\": True, \"has_suspicious_links\": True, \"from_known_sender\": False},\n",
    "    {\"id\": 2, \"contains_promotional_words\": False, \"has_suspicious_links\": False, \"from_known_sender\": True},\n",
    "    {\"id\": 3, \"contains_promotional_words\": True, \"has_suspicious_links\": False, \"from_known_sender\": True},\n",
    "    {\"id\": 4, \"contains_promotional_words\": True, \"has_suspicious_links\": True, \"from_known_sender\": False}\n",
    "]\n",
    "\n",
    "# Example classification\n",
    "classifications = {\n",
    "    1: \"Spam\",      # Has promotional words and suspicious links, not from known sender\n",
    "    2: \"Not Spam\",  # No promotional words or suspicious links, from known sender\n",
    "    3: \"Not Spam\",  # Has promotional words but no suspicious links and from known sender\n",
    "    4: \"Spam\"       # Has promotional words and suspicious links, not from known sender\n",
    "}\n",
    "\n",
    "print(\"Emails and their classifications:\")\n",
    "for email in emails:\n",
    "    email_id = email[\"id\"]\n",
    "    features = f\"Promotional words: {email['contains_promotional_words']}, \" \\\n",
    "              f\"Suspicious links: {email['has_suspicious_links']}, \" \\\n",
    "              f\"Known sender: {email['from_known_sender']}\"\n",
    "    classification = classifications[email_id]\n",
    "    print(f\"Email {email_id}: {features} → Classification: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b86cb1-939b-4870-86b4-24794b922f3e",
   "metadata": {},
   "source": [
    "In this binary classification example, we have two possible classes: 'Spam' or 'Not Spam'. Features like promotional words, suspicious links, and sender reputation helped determine the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b110c-fa43-4dff-8c5f-b4da8ce1887e",
   "metadata": {},
   "source": [
    "### Example 2: Multi-class Classification\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22ab7482-96c3-43cc-9c51-b1181dcc01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plants and their classifications:\n",
      "Plant 1: Height: 15cm, Has flowers: True, Leaf shape: round → Classification: Herb\n",
      "Plant 2: Height: 150cm, Has flowers: False, Leaf shape: needle → Classification: Tree\n",
      "Plant 3: Height: 25cm, Has flowers: True, Leaf shape: oval → Classification: Shrub\n",
      "Plant 4: Height: 5cm, Has flowers: False, Leaf shape: round → Classification: Grass\n"
     ]
    }
   ],
   "source": [
    "# Simple representation of plant features\n",
    "plants = [\n",
    "    {\"id\": 1, \"height_cm\": 15, \"has_flowers\": True, \"leaf_shape\": \"round\"},\n",
    "    {\"id\": 2, \"height_cm\": 150, \"has_flowers\": False, \"leaf_shape\": \"needle\"},\n",
    "    {\"id\": 3, \"height_cm\": 25, \"has_flowers\": True, \"leaf_shape\": \"oval\"},\n",
    "    {\"id\": 4, \"height_cm\": 5, \"has_flowers\": False, \"leaf_shape\": \"round\"}\n",
    "]\n",
    "\n",
    "# Example classification\n",
    "plant_classifications = {\n",
    "    1: \"Herb\",\n",
    "    2: \"Tree\",\n",
    "    3: \"Shrub\",\n",
    "    4: \"Grass\"\n",
    "}\n",
    "\n",
    "print(\"Plants and their classifications:\")\n",
    "for plant in plants:\n",
    "    plant_id = plant[\"id\"]\n",
    "    features = f\"Height: {plant['height_cm']}cm, \" \\\n",
    "              f\"Has flowers: {plant['has_flowers']}, \" \\\n",
    "              f\"Leaf shape: {plant['leaf_shape']}\"\n",
    "    classification = plant_classifications[plant_id]\n",
    "    print(f\"Plant {plant_id}: {features} → Classification: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b509b0-35b6-450a-b9dd-7f2ccaa076df",
   "metadata": {},
   "source": [
    "In this multi-class classification example, we have four possible classes: 'Herb', 'Tree', 'Shrub', or 'Grass'. Features like height, presence of flowers, and leaf shape help determine the class. A machine learning classifier would learn to assign one of these multiple classes based on patterns in features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d54631-e8f3-48ac-b7ae-4808af3496c1",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "## Fully-connected neural network\n",
    "\n",
    "Okay, now that we have some knowledge about the basic terminologies and we have our libraries set up, let us try building our first network. For this, we will use the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) from scikit-learn. Before proceeding make sure to install scikit-learn from your Anaconda Powershell Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c3748be-f5a7-498d-88bd-91d0c7b53229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe008316-1287-4d2c-b7ab-b93c16f3267c",
   "metadata": {},
   "source": [
    "Note: if you find any error message for example saying `No module named 'matplotlib'`, open your Anaconda Powershell Prompt and install the missing library from there. \n",
    "\n",
    "Once installed restart kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aad69b-cade-4cb1-b1e1-3612e4ac1a2b",
   "metadata": {},
   "source": [
    "### Step 1: Load and explore the Iris dataset\n",
    "------------------------------------------\n",
    "The [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) is a classic dataset in machine learning practice containing measurements of sepals and petals from three species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7fb4c2e-05bf-4d49-8884-bdac83596229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "Target classes: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# extract features and target classes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# print to check the overall structure of our dataset\n",
    "# and also to find how many classes we have\n",
    "\n",
    "print(f\"Dataset dimensions: {X.shape}\")\n",
    "print(f\"Target classes: {target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694bdb8-8351-437e-b692-3f08a9deeba4",
   "metadata": {},
   "source": [
    "We now know that we have 150 samples and 4 features in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fb355-cf70-431e-a56f-a590c4482009",
   "metadata": {},
   "source": [
    "### Step 2: Split data into training and testing sets\n",
    "------------------------------------------\n",
    "\n",
    "We now divide our data into training and testing datasets in 80:20 ratio. This means, we will be using 80% of our data for training and 20% for evaluating the model's performance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf5ac41f-706c-4d66-a672-f50f7ba3eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets with a seed for reproducibility\n",
    "# X_train here contains training set for feature data\n",
    "# y_train here contains target labels for training set, or what we want to predict, or the ground truth\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2ed5f-fe74-4615-acec-117101915989",
   "metadata": {},
   "source": [
    "### Step 3: Standarise or scale the feature data\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfe00407-2b92-4514-af17-0627edfb3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the feature data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# learn the parameter from training data and fit a transformer to it\n",
    "# fit() - computes mean and std deviation to scale\n",
    "# transform() - used to scale using mean and std deviation calculated using fit()\n",
    "# fit_transform() - combination of both fit() and transform()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# no fit() as we want to avoid data leakage\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db9496-3489-475b-a919-f31913c766f4",
   "metadata": {},
   "source": [
    "Now let us convert feature matrices to FloatTensor (tensor type for numerical data) and LongTensor (tensor type for integer labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b0d8d01-2769-4b69-9c8d-d18966fd5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52868-f0dc-49af-8afc-3cf0abc692ee",
   "metadata": {},
   "source": [
    "### Step 4: Create tensor dataset and [data loader](https://www.eletreby.me/blog/getting-started-with-pytorch-dataset-and-dataloader) for batch training\n",
    "-------------------------------------------------------\n",
    "\n",
    "The `DataLoader` class wraps the `Dataset` class and handles batching, shuffling, and utilise Python's multiprocessing to speed up data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0052f09-9ce8-42a9-9075-f829c87a0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels into a single dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=len(X_train_tensor), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b2488-1787-479b-b276-a8e086fba259",
   "metadata": {},
   "source": [
    "Finally, our dataset is ready for model definition, training, and evaluation.\n",
    "\n",
    "The following sections will explain the model that we will utilise in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616ec36-50c0-47b0-b6b8-e7efec5b4b22",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "---------------------------------------\n",
    "As we have seen in the previous section, A [multi-layer perceptron](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning) is a type of [feedforward neural network (FNN)](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network) comprised of fully connected neurons with a non-linear activation function. It is commonly employed to differentiate data that cannot be separated linearly.\n",
    "\n",
    "![MLP](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "### Input layer:\n",
    "The input layer serves as the entry point for data into the neural network. Each neuron in this layer represents one feature from our dataset (such as petal length in the iris dataset). These neurons don't perform any computation - they simply pass the input values to the next layer.\n",
    "\n",
    "### Hidden layer:\n",
    "Hidden layers form the core computational engine of the neural network:\n",
    "\n",
    "- Each neuron connects to all neurons in the previous and next layers\n",
    "- These connections have associated weights that determine their importance\n",
    "- The network \"learns\" by adjusting these weights during training\n",
    "- Multiple hidden layers allow the network to build increasingly complex representations\n",
    "\n",
    "### Output layer:\n",
    "The output layer produces the final prediction or classification result. In our example dataset,\n",
    "\n",
    "- Each output neuron typically represents a different class (setosa, versicolor, or virginica in the iris example)\n",
    "- The number of output neurons depends on your specific task\n",
    "\n",
    "#### Workflow:\n",
    "- Information propagates in a forward direction through the network\n",
    "- Within each (artificial) neuron, input signals are aggregated via a weighted sum operation\n",
    "- This aggregated value is then passed through an activation function (introducing non-linearity). Common activation functions include [sigmoid](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/), [tanh](https://www.geeksforgeeks.org/tanh-activation-in-neural-network/), [ReLU (Rectified Linear Unit)](https://medium.com/@gauravnair/the-spark-your-neural-network-needs-understanding-the-significance-of-activation-functions-6b82d5f27fbf#69d4), etc. In this course, we will be using rectified linear unit or ReLU as our activation function.\n",
    "- The resulting output is then forwarded to neurons in the subsequent layer\n",
    "\n",
    "\n",
    "Check out [Neural Network Playground](https://playground.tensorflow.org/) to visualise neural network and play around a bit with features like learning rate, activation, regularization, and problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe7f18-5e55-4803-88f2-1a6ec498a943",
   "metadata": {},
   "source": [
    "## Step 1: Define the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7668a607-4e10-4878-995e-cbcb2c195df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initialise a simple feedforward MLP architecture.\n",
    "        \n",
    "        Parameters:\n",
    "         input_size: Number of input features (e.g., 4 for Iris dataset)\n",
    "         hidden_size: Number of neurons in the hidden layer\n",
    "         num_classes: Number of output classes (e.g., 3 for Iris species)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # First layer (input to hidden)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second layer (hidden to hidden)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer (hidden to output)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass through the network for a single input.\n",
    "        \n",
    "        Parameter:\n",
    "         x: Input tensor of shape [input_size] representing a single sample\n",
    "        \n",
    "        Returns:\n",
    "         Output tensor of shape [num_classes] for a single prediction\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass through the network\n",
    "        # Each step applies a linear transformation followed by a non-linear activation\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3980b7a-7a90-43f8-8635-cee961e494c6",
   "metadata": {},
   "source": [
    "## Step 2: Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45de0023-8692-4fe1-9dcd-2e387fe22f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4    # Assuming 4 features (like Iris dataset)\n",
    "hidden_size = 15  # Neurons in hidden layer\n",
    "num_classes = 3   # Output classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59629b-13c0-4b47-bc98-be6c608ad569",
   "metadata": {},
   "source": [
    "## Step 3: Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bc2760c-34be-41af-a4eb-61821c046e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer1): Linear(in_features=4, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (layer2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (output): Linear(in_features=15, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30083e68-eae0-485b-aba1-08453df5de04",
   "metadata": {},
   "source": [
    "## Step 4: Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5801bf5-f926-411f-abbe-a6a5410233b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current SSE loss: 1.4324\n",
      "Current SSE loss: 1.3866\n",
      "Current SSE loss: 0.5571\n",
      "Current SSE loss: 1.4376\n",
      "Current SSE loss: 1.3976\n",
      "Current SSE loss: 1.5637\n",
      "Current SSE loss: 0.5010\n",
      "Current SSE loss: 1.4522\n",
      "Current SSE loss: 1.4301\n",
      "Current SSE loss: 1.4059\n",
      "Current SSE loss: 1.5602\n",
      "Current SSE loss: 0.4424\n",
      "Current SSE loss: 0.5309\n",
      "Current SSE loss: 1.4000\n",
      "Current SSE loss: 1.4456\n",
      "Current SSE loss: 0.5168\n",
      "Current SSE loss: 1.4834\n",
      "Current SSE loss: 1.4882\n",
      "Current SSE loss: 0.5446\n",
      "Current SSE loss: 1.3786\n",
      "Current SSE loss: 0.4514\n",
      "Current SSE loss: 1.3389\n",
      "Current SSE loss: 0.4205\n",
      "Current SSE loss: 1.4467\n",
      "Current SSE loss: 1.3262\n",
      "Current SSE loss: 0.4913\n",
      "Current SSE loss: 1.4641\n",
      "Current SSE loss: 1.4273\n",
      "Current SSE loss: 1.4041\n",
      "Current SSE loss: 0.5863\n",
      "Current SSE loss: 1.5350\n",
      "Current SSE loss: 1.4361\n",
      "Current SSE loss: 1.4168\n",
      "Current SSE loss: 1.4230\n",
      "Current SSE loss: 0.4549\n",
      "Current SSE loss: 1.4557\n",
      "Current SSE loss: 0.4704\n",
      "Current SSE loss: 1.2869\n",
      "Current SSE loss: 1.4077\n",
      "Current SSE loss: 0.5058\n",
      "Current SSE loss: 1.5795\n",
      "Current SSE loss: 1.3961\n",
      "Current SSE loss: 1.5424\n",
      "Current SSE loss: 1.5602\n",
      "Current SSE loss: 0.4874\n",
      "Current SSE loss: 0.4093\n",
      "Current SSE loss: 1.5006\n",
      "Current SSE loss: 0.4753\n",
      "Current SSE loss: 1.3867\n",
      "Current SSE loss: 0.5863\n",
      "Current SSE loss: 1.5380\n",
      "Current SSE loss: 1.4609\n",
      "Current SSE loss: 1.4471\n",
      "Current SSE loss: 0.5011\n",
      "Current SSE loss: 0.5035\n",
      "Current SSE loss: 1.4232\n",
      "Current SSE loss: 1.5346\n",
      "Current SSE loss: 1.4488\n",
      "Current SSE loss: 1.4037\n",
      "Current SSE loss: 0.4126\n",
      "Current SSE loss: 0.5683\n",
      "Current SSE loss: 1.4647\n",
      "Current SSE loss: 0.4352\n",
      "Current SSE loss: 1.5308\n",
      "Current SSE loss: 1.3555\n",
      "Current SSE loss: 0.4649\n",
      "Current SSE loss: 1.3434\n",
      "Current SSE loss: 1.3842\n",
      "Current SSE loss: 1.5797\n",
      "Current SSE loss: 1.3424\n",
      "Current SSE loss: 1.3892\n",
      "Current SSE loss: 1.4320\n",
      "Current SSE loss: 1.3695\n",
      "Current SSE loss: 0.4770\n",
      "Current SSE loss: 1.4931\n",
      "Current SSE loss: 1.4338\n",
      "Current SSE loss: 1.4843\n",
      "Current SSE loss: 1.5109\n",
      "Current SSE loss: 1.4153\n",
      "Current SSE loss: 0.4704\n",
      "Current SSE loss: 0.5163\n",
      "Current SSE loss: 1.5361\n",
      "Current SSE loss: 0.5921\n",
      "Current SSE loss: 1.5066\n",
      "Current SSE loss: 1.4153\n",
      "Current SSE loss: 1.4601\n",
      "Current SSE loss: 0.4468\n",
      "Current SSE loss: 1.5210\n",
      "Current SSE loss: 0.4917\n",
      "Current SSE loss: 0.4312\n",
      "Current SSE loss: 0.4766\n",
      "Current SSE loss: 1.2270\n",
      "Current SSE loss: 0.5526\n",
      "Current SSE loss: 0.4531\n",
      "Current SSE loss: 1.4387\n",
      "Current SSE loss: 0.4485\n",
      "Current SSE loss: 1.3464\n",
      "Current SSE loss: 1.4475\n",
      "Current SSE loss: 1.4062\n",
      "Current SSE loss: 0.5828\n",
      "Current SSE loss: 1.4756\n",
      "Current SSE loss: 1.3220\n",
      "Current SSE loss: 1.4377\n",
      "Current SSE loss: 1.5140\n",
      "Current SSE loss: 1.3998\n",
      "Current SSE loss: 0.5515\n",
      "Current SSE loss: 1.3580\n",
      "Current SSE loss: 1.5376\n",
      "Current SSE loss: 0.4570\n",
      "Current SSE loss: 1.4868\n",
      "Current SSE loss: 0.4884\n",
      "Current SSE loss: 0.5326\n",
      "Current SSE loss: 1.5462\n",
      "Current SSE loss: 1.5512\n",
      "Current SSE loss: 1.4737\n",
      "Current SSE loss: 0.4997\n",
      "Current SSE loss: 1.5587\n",
      "Current SSE loss: 1.4476\n",
      "Current SSE loss: 0.4865\n",
      "Current SSE loss: 1.4297\n",
      "Average SSE loss across 120 samples: 1.1208\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(model, X, y_true):\n",
    "    \"\"\"\n",
    "    Calculate loss for the model without training.\n",
    "    \n",
    "    Parameters:\n",
    "     model: The MLP model\n",
    "     X: Input features (torch tensor) - single sample, not batched\n",
    "     y_true: True label (torch tensor) - single label, not batched\n",
    "    \n",
    "    Returns:\n",
    "     loss: Sum of squared errors loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add batch dimension of 1 for model compatibility\n",
    "    X_input = X.unsqueeze(0)  # Adds batch dimension [1, input_features]\n",
    "    y_pred = model(X_input)   # y_pred shape: [1, num_classes]\n",
    "    \n",
    "    # Get number of classes from model's output layer\n",
    "    num_classes = model.output.out_features\n",
    "    \n",
    "    # Create one-hot encoded label for a single sample\n",
    "    \"\"\"\n",
    "    as we are working with a classification problem that has 3 possible classes (0, 1, and 2). \n",
    "    Using one-hot encoding gives us:\n",
    "\n",
    "    Class 0 becomes: [1, 0, 0, 0]\n",
    "    Class 1 becomes: [0, 1, 0, 0]\n",
    "    Class 2 becomes: [0, 0, 1, 0]\n",
    "\n",
    "    \"\"\"\n",
    "    y_true_one_hot = torch.zeros(1, num_classes)\n",
    "    \n",
    "    # Convert label tensor to integer and set the appropriate position to 1\n",
    "    label_idx = y_true.item()\n",
    "    y_true_one_hot[0, label_idx] = 1\n",
    "    \n",
    "    # Calculate sum of squared errors between prediction and one-hot label\n",
    "    squared_errors = (y_pred - y_true_one_hot) ** 2\n",
    "    loss = torch.sum(squared_errors)\n",
    "    \n",
    "    # Calculate and print the loss value\n",
    "    loss_value = loss.item()\n",
    "    print(f\"Current SSE loss: {loss_value:.4f}\")\n",
    "    \n",
    "    # Return the loss as a Python float\n",
    "    return loss_value\n",
    "###############################################################\n",
    "# Usage with individual data points\n",
    "total_loss = 0\n",
    "num_samples = 0\n",
    "\n",
    "# Iterate through the dataset individually\n",
    "for features, labels in train_loader:\n",
    "    \n",
    "    for i in range(features.size(0)):\n",
    "        \n",
    "        # Extract individual feature and label\n",
    "        single_feature = features[i]  # Single feature\n",
    "        single_label = labels[i]      # Single label\n",
    "        \n",
    "        # Calculate loss for individual sample\n",
    "        loss = calculate_loss(model, single_feature, single_label)\n",
    "        total_loss += loss\n",
    "        num_samples += 1\n",
    "\n",
    "# Calculate average loss across all processed samples\n",
    "if num_samples > 0:\n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(f\"Average SSE loss across {num_samples} samples: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e316e-075b-4798-b1c6-6176097731a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-CPU",
   "language": "python",
   "name": "py312-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
