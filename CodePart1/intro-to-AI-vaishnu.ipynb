{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c4a454-d0f4-4d66-af7b-f59703d7f53a",
   "metadata": {},
   "source": [
    "# Basics of installing PyTorch (CUDA) in Anaconda\n",
    "- Open Anaconda Powershell Prompt\n",
    "- Create new virtual environment: conda create -n py312 python=3.12\n",
    "- Activate it: conda activate py312\n",
    "- Install PyTorch using CONDA: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "- Verify PyTorch installation: python -c \"import torch; print(torch.__ version__)\"\n",
    "- Verify CUDA availability: python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "### Alternative PyTorch\n",
    "- Install PyTorch for CPU: conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4fdeb-283d-45ab-a3c1-5fc898500cec",
   "metadata": {},
   "source": [
    "## Tips to redirect your Jupyter Notebook kernel to the new environment\n",
    "Activate your virtual environment first on Anaconda Powershell Prompt\n",
    "- #### Install ipykernel\n",
    "conda install ipykernel\n",
    "\n",
    "- #### Add the environment to Jupyter (e.g. if your virtual environment name is py312)\n",
    "python -m ipykernel install --user --name=py312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2df348-a993-4323-89dd-40510fafa5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if PyTorch exists otherwise follow the above steps to install PyTorch\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e82fc-949c-403e-b4e2-42de07a5502c",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "A tensor can be viewed as a multi-dimensional array. Similar to how an n-dimensional vector is shown as a one-dimensional array with _n_ elements relative to a specific basis, any tensor can be expressed as a multi-dimensional array when referenced to a basis. The individual values within this multi-dimensional structure are referred to as the tensor's components.\n",
    "\n",
    "The PyTorch library offers multi-dimensional tensor data structures and implements various mathematical functions to manipulate these tensors. It also includes numerous tools for effective tensor serialisation, handling arbitrary data types, and provides several other practical utilities.\n",
    "\n",
    "PyTorch shares significant similarities with NumPy, though it uses the term ''tensor'' instead of ''N-dimensional array''. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5e8928-5f2d-42a3-be42-5717dd3f64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "array_np = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "array_pytorch = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6]])\n",
    "print(array_np)\n",
    "print(array_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3a656-4d05-48c0-b7fe-bcbcee088e81",
   "metadata": {},
   "source": [
    "A parallel [PyTorch CUDA](https://pytorch.org/docs/2.5/cuda.html) version is also available, allowing you to execute tensor calculations on NVIDIA GPUs that have a compute capability of 3.0 or higher. But in this course, as time may not permit, we will be restricted to pre-definied dataset. In future, your project might need CUDA acceleration for which please install the CUDA version of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0520-6a94-4be8-ba6d-418625907e48",
   "metadata": {},
   "source": [
    "## Classes\n",
    "\n",
    "Classifications (classes) usually refers to catergories or labels our neural network is supposed to predict.\n",
    "\n",
    "It can be of two types: \n",
    "- binary classification (yes or no/malignant or benign/dog or cat etc.) or\n",
    "- multi-class classification (cat or dog or capibara/digit recognition/species of flowers etc.) .\n",
    "\n",
    "Okay, now that we have some knowledge about the basic terminologies and we have our libraries set up, let us try building our first network.\n",
    "\n",
    "For this, we will use the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) from scikit-learn.\n",
    "\n",
    "Before proceeding make sure to install scikit-learn from your Anaconda Powershell Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3748be-f5a7-498d-88bd-91d0c7b53229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe008316-1287-4d2c-b7ab-b93c16f3267c",
   "metadata": {},
   "source": [
    "Note: if you find any error message for example saying `No module named 'matplotlib'`, open your Anaconda Powershell Prompt and install the missing library from there. \n",
    "\n",
    "Once installed restart kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aad69b-cade-4cb1-b1e1-3612e4ac1a2b",
   "metadata": {},
   "source": [
    "### Step 1: Load and explore the Iris dataset\n",
    "------------------------------------------\n",
    "The [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) is a classic dataset in machine learning practice containing measurements of sepals and petals from three species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fb4c2e-05bf-4d49-8884-bdac83596229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "Target classes: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# extract features and target classes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# print to check the overall structure of our dataset\n",
    "# and also to find how many classes we have\n",
    "\n",
    "print(f\"Dataset dimensions: {X.shape}\")\n",
    "print(f\"Target classes: {target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694bdb8-8351-437e-b692-3f08a9deeba4",
   "metadata": {},
   "source": [
    "We now know that we have 150 samples and 4 features in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fb355-cf70-431e-a56f-a590c4482009",
   "metadata": {},
   "source": [
    "### Step 2: Split data into training and testing sets\n",
    "------------------------------------------\n",
    "\n",
    "We now divide our data into training and testing datasets in 80:20 ratio. This means, we will be using 80% of our data for training and 20% for evaluating the model's performance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5ac41f-706c-4d66-a672-f50f7ba3eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets with a seed for reproducibility\n",
    "# X_train here contains training set for feature data\n",
    "# y_train here contains target labels for training set, or what we want to predict, or the ground truth\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2ed5f-fe74-4615-acec-117101915989",
   "metadata": {},
   "source": [
    "### Step 3: Standarise or scale the feature data\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe00407-2b92-4514-af17-0627edfb3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the feature data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# learn the parameter from training data and fit a transformer to it\n",
    "# fit() - computes mean and std deviation to scale\n",
    "# transform() - used to scale using mean and std deviation calculated using fit()\n",
    "# fit_transform() - combination of both fit() and transform()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# no fit() as we want to avoid data leakage\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db9496-3489-475b-a919-f31913c766f4",
   "metadata": {},
   "source": [
    "Now let us convert feature matrices to FloatTensor (tensor type for numerical data) and LongTensor (tensor type for integer labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0d8d01-2769-4b69-9c8d-d18966fd5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52868-f0dc-49af-8afc-3cf0abc692ee",
   "metadata": {},
   "source": [
    "### Step 4: Create tensor dataset and [data loader](https://www.eletreby.me/blog/getting-started-with-pytorch-dataset-and-dataloader) for batch training\n",
    "-------------------------------------------------------\n",
    "\n",
    "The `DataLoader` class wraps the `Dataset` class and handles batching, shuffling, and utilise Python's multiprocessing to speed up data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0052f09-9ce8-42a9-9075-f829c87a0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features and labels into a single dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# create a dataloader \n",
    "# batch_size - instead of processing all training examples at once, it splits them into smaller batches of 16 examples each\n",
    "# shuffle = True - randomise the order of training examples before creating batches\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b2488-1787-479b-b276-a8e086fba259",
   "metadata": {},
   "source": [
    "Finally, our dataset is ready for model definition, training, and evaluation.\n",
    "\n",
    "The following sections will explain the model that we will utilise in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616ec36-50c0-47b0-b6b8-e7efec5b4b22",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "---------------------------------------\n",
    "A [multi-layer perceptron](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning) is a type of feedforward neural network (FNN) comprised of fully connected neurons with a non-linear activation function. It is commonly employed to differentiate data that cannot be separated linearly.\n",
    "![MLP](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "### Input layer:\n",
    "This is where the data enters and each neuron represents one piece of information (e.g. petal length)\n",
    "\n",
    "### Hidden layer:\n",
    "This is where real work happens. Each neuron here connects to input and output layer where the connections have weights. These weights resembles importance of some connections over others.\n",
    "\n",
    "### Output layer:\n",
    "This is final layer where we get our results. In our iris dataset example, we want these neurons to represent a possible class (setosa, versicolor, or virginica).\n",
    "\n",
    "#### Workflow:\n",
    "- Information propagates in a forward direction through the network\n",
    "- Within each (artificial) neuron, input signals are aggregated via a weighted sum operation\n",
    "- This aggregated value is then passed through an activation function (introducing non-linearity). Common activation functions include [sigmoid](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/), [tanh](https://www.geeksforgeeks.org/tanh-activation-in-neural-network/), [ReLU (Rectified Linear Unit)](https://medium.com/@gauravnair/the-spark-your-neural-network-needs-understanding-the-significance-of-activation-functions-6b82d5f27fbf#69d4), etc.\n",
    "- The resulting output is then forwarded to neurons in the subsequent layer\n",
    "\n",
    "\n",
    "Check out [Neural Network Playground](https://playground.tensorflow.org/) to visualise neural network and play around a bit with features like learning rate, activation, regularization, and problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe7f18-5e55-4803-88f2-1a6ec498a943",
   "metadata": {},
   "source": [
    "## Step 1: Define the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7668a607-4e10-4878-995e-cbcb2c195df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initialise the MLP architecture with specified dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "         input_size: Number of input features (4 for Iris dataset)\n",
    "         hidden_size: Number of neurons in the hidden layer\n",
    "         num_classes: Number of output classes (3 for Iris species)\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # First layer (input to hidden)\n",
    "        # Linear transformation from input features to hidden neurons\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second layer (hidden to hidden)\n",
    "        # Another hidden layer with the same size for more complex pattern recognition\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer (hidden to output)\n",
    "        # Maps from hidden representation to class scores (logits)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass through the network.\n",
    "        \n",
    "        Parameter:\n",
    "         x: Input tensor of shape [batch_size, input_size]\n",
    "        \n",
    "        Returns:\n",
    "         Output tensor of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        # Each step applies a linear transformation followed by a non-linear activation\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3980b7a-7a90-43f8-8635-cee961e494c6",
   "metadata": {},
   "source": [
    "## Step 2: Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45de0023-8692-4fe1-9dcd-2e387fe22f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of features (4 for Iris)\n",
    "hidden_size = 10               # Number of neurons in hidden layer\n",
    "num_classes = 3                # Number of output classes (3 for Iris)\n",
    "learning_rate = 0.01           # Learning rate for optimiser\n",
    "num_epochs = 100               # Number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59629b-13c0-4b47-bc98-be6c608ad569",
   "metadata": {},
   "source": [
    "## Step 3: Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc2760c-34be-41af-a4eb-61821c046e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (output): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30083e68-eae0-485b-aba1-08453df5de04",
   "metadata": {},
   "source": [
    "## Step 4: Loss function and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5801bf5-f926-411f-abbe-a6a5410233b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3057c-2bd0-420f-8d89-c43d50a4d9e5",
   "metadata": {},
   "source": [
    "`criterion = nn.CrossEntropyLoss()`\n",
    "- Defines our loss function, which measures how far our predictions deviate from the actual labels\n",
    "- [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) is ideal for multi-class classification (such as our Iris dataset)\n",
    "- It combines softmax activation and negative log-likelihood loss in a single, numerically stable function\n",
    "- It expects raw model outputs ([logits](https://www.columbia.edu/~so33/SusDev/Lecture_9.pdf)) rather than probabilities\n",
    "\n",
    "`optimiser = optim.Adam(model.parameters(), lr=learning_rate)`\n",
    "- This creates an optimiser that will update our model's weights\n",
    "- [Adam (Adaptive Moment Estimation)](https://arxiv.org/abs/1412.6980) is a popular optimiser that:\n",
    "     - Adapts the learning rate for each parameter\n",
    "     - Combines the benefits of [AdaGrad](https://medium.com/@brijesh_soni/understanding-the-adagrad-optimization-algorithm-an-adaptive-learning-rate-approach-9dfaae2077bb) and [RMSProp](https://medium.com/@nerdjock/deep-learning-course-lesson-7-3-rmsprop-root-mean-square-propagation-7ff9a3ae2cca)\n",
    "     - Works well for most problems without requiring excessive tuning\n",
    "- model.parameters() gives the optimiser access to all trainable weights in our network\n",
    "- lr=learning_rate sets the learning rate (how significant each update step should be)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2e62db0-0487-4b32-8e23-aef203aefe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Training Loss: 0.1442, Test Loss: 0.0901\n",
      "Epoch 20/100, Training Loss: 0.0597, Test Loss: 0.0356\n",
      "Epoch 30/100, Training Loss: 0.0465, Test Loss: 0.0201\n",
      "Epoch 40/100, Training Loss: 0.0485, Test Loss: 0.0336\n",
      "Epoch 50/100, Training Loss: 0.0401, Test Loss: 0.0143\n",
      "Epoch 60/100, Training Loss: 0.0406, Test Loss: 0.0148\n",
      "Epoch 70/100, Training Loss: 0.0447, Test Loss: 0.0113\n",
      "Epoch 80/100, Training Loss: 0.0402, Test Loss: 0.0112\n",
      "Epoch 90/100, Training Loss: 0.0463, Test Loss: 0.0099\n",
      "Epoch 100/100, Training Loss: 0.0400, Test Loss: 0.0093\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Lists to track loss values\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimise\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for this epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor).item()\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44a38c-5f14-489f-9336-e376f26788d3",
   "metadata": {},
   "source": [
    "## Step 5: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bad229-820f-4273-bb28-75d11075744f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c8158-0625-4856-9841-0f45545a4bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-CPU",
   "language": "python",
   "name": "py312-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
